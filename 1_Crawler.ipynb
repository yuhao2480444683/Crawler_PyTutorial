{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检测网站使用技术\n",
    "\n",
    "#import builtwith\n",
    "#builtwith.parse('https://docs.microsoft.com/en-us/xamarin/xamarin-forms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检测网站拥有者\n",
    "\n",
    "#import whois\n",
    "#print(whois.whois('https://docs.microsoft.com/en-us/xamarin/xamarin-forms') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 网页下载 （捕获异常、服务器错误500重试下载、用户代理）\n",
    "import urllib\n",
    "def download(url, user_agent='GoodCrawler', num_retries=2):\n",
    "    print ('Downloading:',url)\n",
    "    # 创建请求而不是url， url读取会被禁止, 设置代理(默认代理会被拒)\n",
    "    req = urllib.request.Request(url, headers={'User-agent':user_agent}) \n",
    "    try:\n",
    "        # 读取后编码成字符串，在后面re.findall中才能匹配\n",
    "        html = urllib.request.urlopen(req).read().decode('utf-8')\n",
    "    except urllib.error.HTTPError as e:\n",
    "        print ('Download error:',e.reason)\n",
    "        html = None\n",
    "        if(num_retries > 0):\n",
    "            # 处理500-600的服务器错误\n",
    "            if hasattr(e, 'code') and 500 <= e.code <600:\n",
    "                return download(url, num_retries - 1)\n",
    "            \n",
    "    return html\n",
    "\n",
    "#download('http://httpstat.us/500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过sitemap爬取网页\n",
    "import re\n",
    "def crawl_sitemap(url): \n",
    "    # download the sitemap file \n",
    "    sitemap = download(url) \n",
    "    # extract the sitemap links \n",
    "    links = re.findall(r\"<loc>(.*?)</loc>\", sitemap) \n",
    "    # download each link \n",
    "    for link in links: \n",
    "        html = download(link)\n",
    "        # scrape html here\n",
    "\n",
    "#crawl_sitemap('http://example.webscraping.com/sitemap.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n#最大下载错误数量\\nmax_errors = 5\\n#当前下载错误数量\\nnum_errors = 0\\n\\nfor page in itertools.count(1):\\n    # 示例网站随时可能变化，可以换一个随意测试\\n    url = 'http://example.webscraping.com/places/default/view/%d' % page\\n    html = download(url)\\n    if html is None:\\n        num_errors += 1\\n        if num_errors == max_errors:\\n            break\\n    else:\\n        # success - can scrape the result\\n        num_errors = 0\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ID遍历爬虫\n",
    "    # Id可能不连续，因此增加多次下载错误后才退出程序\n",
    "import itertools\n",
    "\n",
    "'''\n",
    "\n",
    "#最大下载错误数量\n",
    "max_errors = 5\n",
    "#当前下载错误数量\n",
    "num_errors = 0\n",
    "\n",
    "for page in itertools.count(1):\n",
    "    # 示例网站随时可能变化，可以换一个随意测试\n",
    "    url = 'http://example.webscraping.com/places/default/view/%d' % page\n",
    "    html = download(url)\n",
    "    if html is None:\n",
    "        num_errors += 1\n",
    "        if num_errors == max_errors:\n",
    "            break\n",
    "    else:\n",
    "        # success - can scrape the result\n",
    "        num_errors = 0\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 链接爬虫1.0(去重、绝对/相对路径<urljoin>)\n",
    "import re\n",
    "# 链接爬取   参数：1.根url    2.正则匹配表达式\n",
    "def link_crawler(seed_url, link_regex):\n",
    "    crawl_queue = [seed_url]\n",
    "    # 避免重复url\n",
    "    seen = set(crawl_queue)\n",
    "    while crawl_queue:\n",
    "        url = crawl_queue.pop()\n",
    "        html = download(url)\n",
    "        \n",
    "        for link in get_links(html):\n",
    "            if re.match(link_regex, link):\n",
    "                link = urllib.parse.urljoin(seed_url, link)\n",
    "                if link not in seen:\n",
    "                    seen.add(link)\n",
    "                    crawl_queue.append(link)\n",
    "                \n",
    "def get_links(html):\n",
    "    webpage_regex = re.compile('<a[^>]+href=[\"\\'](.*?)[\"\\']', re.IGNORECASE)\n",
    "    return webpage_regex.findall(html)\n",
    "\n",
    "#link_crawler('http://example.webscraping.com', '/(places/default/view|places/default/index)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 判断是否可以爬取网页\n",
    "\n",
    "import urllib.robotparser as r_p\n",
    "rp = r_p.RobotFileParser()\n",
    "rp.set_url('http://example.webscraping.com/robots.txt')\n",
    "rp.read()\n",
    "url = 'http://example.webscraping.com'\n",
    "'''\n",
    "user_agent = 'BadCrawler'\n",
    "rp.can_fetch(user_agent, url)\n",
    "'''\n",
    "user_agent = 'GoodCrawler'\n",
    "rp.can_fetch(user_agent, url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Downloading: http://example.webscraping.com\n",
      "True\n",
      "Downloading: http://example.webscraping.com/places/default/index/1\n",
      "True\n",
      "Downloading: http://example.webscraping.com/places/default/view/Antigua-and-Barbuda-10\n",
      "True\n",
      "Downloading: http://example.webscraping.com/places/default/view/Antarctica-9\n",
      "True\n",
      "Downloading: http://example.webscraping.com/places/default/view/Anguilla-8\n",
      "True\n",
      "Downloading: http://example.webscraping.com/places/default/view/Angola-7\n",
      "True\n",
      "Downloading: http://example.webscraping.com/places/default/view/Andorra-6\n",
      "True\n",
      "Downloading: http://example.webscraping.com/places/default/view/American-Samoa-5\n",
      "True\n",
      "Downloading: http://example.webscraping.com/places/default/view/Algeria-4\n",
      "True\n",
      "Downloading: http://example.webscraping.com/places/default/view/Albania-3\n",
      "True\n",
      "Downloading: http://example.webscraping.com/places/default/view/Aland-Islands-2\n",
      "True\n",
      "Downloading: http://example.webscraping.com/places/default/view/Afghanistan-1\n",
      "True\n",
      "Downloading: http://example.webscraping.com/places/default/index\n"
     ]
    }
   ],
   "source": [
    "# 链接爬虫2.0 \n",
    "    # 加入禁止爬取的robots.txt判断\n",
    "    # 加入爬虫限速\n",
    "    # 避免爬虫陷阱，设置到达深度（本例子默认只下载一个页面）\n",
    "    \n",
    "    \n",
    "# 爬虫限速\n",
    "# Throttle类记录每个域名最近访问时间\n",
    "class Throttle:\n",
    "    def __init__(self, delay):\n",
    "        self.delay = delay\n",
    "        self.domains = {}\n",
    "    def wait(self, url):\n",
    "        domain = urllib.parse.urlparse(url).netloc\n",
    "        last_accessed = self.domains.get(domain)\n",
    "        if self.delay > 0 and last_accessed is not None:\n",
    "            sleep_secs = self.delay - (datetime.datetime.now() - last_accessed).seconds\n",
    "            if sleep_secs > 0:\n",
    "                time.sleep(sleep_secs)\n",
    "        self.domains[domain] = datetime.datetime.now()    \n",
    "\n",
    "        \n",
    "import re\n",
    "import datetime\n",
    "import urllib.robotparser as r_p\n",
    "rp = r_p.RobotFileParser()\n",
    "rp.set_url('http://example.webscraping.com/robots.txt')\n",
    "rp.read()\n",
    "\n",
    "user_agent = 'GoodCrawler'\n",
    "\n",
    "# 链接爬取   参数：1.根url    2.正则匹配表达式\n",
    "def link_crawler2(seed_url, link_regex, max_depth=2):\n",
    "    max_depth = 2\n",
    "    crawl_queue = [seed_url]\n",
    "    # 避免重复url\n",
    "    seen = {}\n",
    "    seen[seed_url] = 1\n",
    "    while crawl_queue:\n",
    "        url = crawl_queue.pop()\n",
    "        print( rp.can_fetch(user_agent, url))\n",
    "        # 检查robots.txt是否当前代理可以爬取\n",
    "        if not rp.can_fetch(user_agent, url):\n",
    "            print(\"Blocked by robots.txt\", user_agent, url)\n",
    "            return\n",
    "        throttle = Throttle(5)\n",
    "        throttle.wait(url)\n",
    "        html = download(url)\n",
    "        \n",
    "        depth = seen[url]\n",
    "        if depth != max_depth:\n",
    "            for link in get_links(html):\n",
    "                if re.match(link_regex, link):\n",
    "                    link = urllib.parse.urljoin(seed_url, link)\n",
    "                    if link not in seen:\n",
    "                        seen[link] = depth + 1\n",
    "                        crawl_queue.append(link)\n",
    "\n",
    "def get_links(html):\n",
    "    webpage_regex = re.compile('<a[^>]+href=[\"\\'](.*?)[\"\\']', re.IGNORECASE)\n",
    "    return webpage_regex.findall(html)\n",
    "\n",
    "link_crawler2('http://example.webscraping.com', '/(places/default/view|places/default/index)')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
