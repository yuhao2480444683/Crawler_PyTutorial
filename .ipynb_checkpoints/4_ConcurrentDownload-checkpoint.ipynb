{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloader类\n",
    "    \n",
    "import urllib\n",
    "class Downloader:\n",
    "    def __init__(self, delay=5, user_agent='yuhao', num_retries=1, cache=None):\n",
    "        self.throttle = Throttle(delay)\n",
    "        self.user_agent = user_agent\n",
    "        self.num_retries = num_retries\n",
    "        self.cache = cache\n",
    "    def __call__(self, url):\n",
    "        result = None\n",
    "        if self.cache:\n",
    "            try:\n",
    "                result = self.cache[url]\n",
    "            except KeyError:\n",
    "                #url is not available in cache\n",
    "                pass\n",
    "            else:\n",
    "                if result['code'] == None:\n",
    "                     return result['html']\n",
    "                if self.num_retries > 0 and 500 <= result['code'] <600:\n",
    "                    result = None\n",
    "        if result is None:\n",
    "            self.throttle.wait(url)\n",
    "            headers = {'User-agent':self.user_agent}\n",
    "            result = self.download(url, headers, self.num_retries)\n",
    "            if self.cache:\n",
    "                self.cache[url] = result\n",
    "            return result['html']\n",
    "    def download(self, url, headers, num_retries, data=None):\n",
    "        print ('Downloading:',url)\n",
    "        # 创建请求而不是url， url读取会被禁止, 设置代理(默认代理会被拒)\n",
    "        req = urllib.request.Request(url, headers={'User-agent':user_agent})\n",
    "        code = None\n",
    "        try:\n",
    "            # 读取后编码成字符串，在后面re.findall中才能匹配\n",
    "            html = urllib.request.urlopen(req).read().decode('utf-8')\n",
    "\n",
    "        except urllib.error.HTTPError as e:\n",
    "            print ('Download error:',e.reason)\n",
    "            html = None\n",
    "            code = e.code\n",
    "            if(num_retries > 0):\n",
    "                # 处理500-600的服务器错误\n",
    "                if hasattr(e, 'code') and 500 <= e.code <600:\n",
    "                    return download(url, num_retries - 1)\n",
    "        \n",
    "        return {'html':html,'code':code}\n",
    "        \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用MongoDB实现cache类\n",
    "    # 创建index时，expireAfterSeconds属性如果为utc时间，则会被数据库自动删除，否则不会\n",
    "from datetime import datetime, timedelta\n",
    "from pymongo import MongoClient\n",
    "\n",
    "class MongoCache:\n",
    "    def __init__(self, client=None, expires=timedelta(days=30)):\n",
    "        self.client = MongoClient('localhost',27017)\n",
    "        self.db = client.cache\n",
    "        self.db.webpage.create_index('timestamp', expireAfterSeconds=expires.total_seconds())\n",
    "    \n",
    "    def __getitem__(self, url):\n",
    "        record = self.db.webpage.find_one({'_id':url})\n",
    "        if record:\n",
    "            return record['result']\n",
    "        else:\n",
    "            raise KeyError(url + 'does not exist')\n",
    "            \n",
    "    def __setitem__(self, url, result):\n",
    "        record = {'result': result, 'timestamp':datetime.now()}\n",
    "        self.db.webpage.update( {'_id': url}, {'$set': record},  upsert=True)\n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\nclient = MongoClient('localhost',27017)\\ncache = MongoCache(client=client, expires=timedelta())\\nlink_crawler4('http://example.webscraping.com', '/(places/default/view|places/default/index)', \\n              delay = 3, cache = cache, num_retries = 2,  max_depth = -1)\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 链接爬虫4.0\n",
    "\n",
    "\n",
    "import re\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import urllib.robotparser as r_p\n",
    "import time\n",
    "import lxml.html\n",
    "\n",
    "# Throttle类记录每个域名最近访问时间\n",
    "class Throttle:\n",
    "    def __init__(self, delay):\n",
    "        self.delay = delay\n",
    "        self.domains = {}\n",
    "    def wait(self, url):\n",
    "        domain = urllib.parse.urlparse(url).netloc\n",
    "        last_accessed = self.domains.get(domain)\n",
    "        if self.delay > 0 and last_accessed is not None:\n",
    "            sleep_secs = self.delay - (datetime.now() - last_accessed).seconds\n",
    "            if sleep_secs > 0:\n",
    "                time.sleep(sleep_secs)\n",
    "        self.domains[domain] = datetime.now()    \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "rp = r_p.RobotFileParser()\n",
    "rp.set_url('http://example.webscraping.com/robots.txt')\n",
    "rp.read()\n",
    "\n",
    "user_agent = 'GoodCrawler'\n",
    "\n",
    "\n",
    "\n",
    "# 链接爬取\n",
    "def link_crawler4(seed_url, link_regex, num_retries=1, delay=5, max_depth=2, cache=None):\n",
    "    max_depth = 2\n",
    "    crawl_queue = [seed_url]\n",
    "    # 避免重复url\n",
    "    seen = {seed_url:0}\n",
    "    \n",
    "    num_urls = 0\n",
    "    D= Downloader(delay=delay, user_agent=user_agent, num_retries=num_retries, cache=cache)\n",
    "    \n",
    "    while crawl_queue:\n",
    "        url = crawl_queue.pop()\n",
    "        depth = seen[url]\n",
    "        print( rp.can_fetch(user_agent, url))\n",
    "        # 检查robots.txt是否当前代理可以爬取\n",
    "        if not rp.can_fetch(user_agent, url):\n",
    "            print(\"Blocked by robots.txt\", user_agent, url)\n",
    "            return\n",
    "        html = D(url)\n",
    "        \n",
    "        if depth != max_depth:\n",
    "            for link in get_links(html):\n",
    "                if re.match(link_regex, link):\n",
    "                    link = urllib.parse.urljoin(seed_url, link)\n",
    "                    if link not in seen:\n",
    "                        seen[link] = depth + 1\n",
    "                        crawl_queue.append(link)\n",
    "\n",
    "def get_links(html):\n",
    "    webpage_regex = re.compile('<a[^>]+href=[\"\\'](.*?)[\"\\']', re.IGNORECASE)\n",
    "    return webpage_regex.findall(html)\n",
    "\n",
    "\n",
    "''' \n",
    "client = MongoClient('localhost',27017)\n",
    "cache = MongoCache(client=client, expires=timedelta())\n",
    "link_crawler4('http://example.webscraping.com', '/(places/default/view|places/default/index)', \n",
    "              delay = 3, cache = cache, num_retries = 2,  max_depth = -1)\n",
    "'''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# 下载用于并发例子的url数据\\nimport csv\\nfrom zipfile import ZipFile\\n\\nD = Downloader()\\nzipped_data = D('http://s3.amazonaws.com/alexa-static/top-1m.csv.zip')\\nurls = []  # top 1 million URL will be stored in this list\\n    # ZipFile需要一个文件接口，不能是字符串，用StringIO包装\\nwith ZipFile(io.StringIO(zipped_data)) as zf:       \\n    csv_filename = zf.namelist()[0]\\n    for _, website in csv.reader(zf.open(csv_filename)):\\n        urls.append('http://' + website)\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# 下载用于并发例子的url数据\n",
    "import csv\n",
    "from zipfile import ZipFile\n",
    "\n",
    "D = Downloader()\n",
    "zipped_data = D('http://s3.amazonaws.com/alexa-static/top-1m.csv.zip')\n",
    "urls = []  # top 1 million URL will be stored in this list\n",
    "    # ZipFile需要一个文件接口，不能是字符串，用StringIO包装\n",
    "with ZipFile(io.StringIO(zipped_data)) as zf:       \n",
    "    csv_filename = zf.namelist()[0]\n",
    "    for _, website in csv.reader(zf.open(csv_filename)):\n",
    "        urls.append('http://' + website)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修改scrape_callback接口\n",
    "\n",
    "class AlexaCallback:\n",
    "    def __init__(self, max_urls=1000):\n",
    "        self.max_urls = max_urls    # 设定Alexa文件中提取的URL数量\n",
    "        self.seed_url = 'http://s3.amazonaws.com/alexa-static/top-1m.csv.zip'\n",
    "        \n",
    "    def __call__(self, url, html):\n",
    "        if url == self.seed_url:\n",
    "            urls = []\n",
    "            with ZipFile(StringIO(zipped_data)) as zf:       \n",
    "                csv_filename = zf.namelist()[0]\n",
    "                for _, website in csv.reader(zf.open(csv_filename)):\n",
    "                    urls.append('http://' + website)\n",
    "                    if len(urls) == self.max_urls:\n",
    "                        break\n",
    "            return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# 串行下载，使用Alexa回调函数\\n\\n# 串行下载估算，每个URL下载花费1.6秒\\nscrape_callback = AlexaCallback()\\nlink_crawler4(seed_url=scrape_callback.seed_url, cache_callback=MongoCache(), scrape_callback=scrape_callback)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# 串行下载，使用Alexa回调函数\n",
    "\n",
    "# 串行下载估算，每个URL下载花费1.6秒\n",
    "scrape_callback = AlexaCallback()\n",
    "link_crawler4(seed_url=scrape_callback.seed_url, cache_callback=MongoCache(), scrape_callback=scrape_callback)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: http://example.webscraping.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Root\\Anaconda3\\envs\\paddlepaddle\\lib\\site-packages\\ipykernel_launcher.py:21: DeprecationWarning: update is deprecated. Use replace_one, update_one or update_many instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: http://example.webscraping.com/places/default/view/Antigua-and-Barbuda-10\n",
      "Downloading:Downloading: http://example.webscraping.com/places/default/view/Angola-7\n",
      "Downloading: http://example.webscraping.com/places/default/view/Antarctica-9\n",
      "Downloading: http://example.webscraping.com/places/default/view/Albania-3\n",
      " Downloading: http://example.webscraping.com/places/default/view/Aland-Islands-2Downloading: http://example.webscraping.com/places/default/view/Andorra-6\n",
      "Downloading: http://example.webscraping.com/places/default/view/Algeria-4\n",
      "Downloading: http://example.webscraping.com/places/default/view/American-Samoa-5\n",
      "http://example.webscraping.com/places/default/view/Anguilla-8\n",
      "\n",
      "Downloading: http://example.webscraping.com/places/default/index/1\n",
      "Downloading: http://example.webscraping.com/places/default/index\n",
      "Downloading:Downloading:  http://example.webscraping.com/places/default/index/0\n",
      "http://example.webscraping.com/places/default/index/2\n",
      "Downloading: http://example.webscraping.com/places/default/view/Barbados-20\n",
      "Downloading: http://example.webscraping.com/places/default/view/Bangladesh-19\n",
      "Downloading: http://example.webscraping.com/places/default/view/Bahrain-18\n",
      "Downloading:Downloading:  http://example.webscraping.com/places/default/view/Bahamas-17\n",
      "http://example.webscraping.com/places/default/view/Azerbaijan-16\n",
      "Downloading: http://example.webscraping.com/places/default/view/Afghanistan-1\n",
      "Downloading: http://example.webscraping.com/places/default/view/Austria-15\n",
      "Downloading: http://example.webscraping.com/places/default/view/Aruba-13\n",
      "Downloading: http://example.webscraping.com/places/default/view/Armenia-12\n",
      "Downloading: http://example.webscraping.com/places/default/view/Argentina-11\n",
      "Downloading: http://example.webscraping.com/places/default/view/Australia-14\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 多线程爬虫\n",
    "\n",
    "\n",
    "import time\n",
    "import threading\n",
    "SLEEP_TIME = 1\n",
    "\n",
    "def threaded_crawler(seed_url, link_regex, num_retries=1, delay=5, max_depth=3, cache=None , max_threads=10):\n",
    "\n",
    "    crawl_queue = [seed_url]\n",
    "    # 避免重复url\n",
    "    seen = {seed_url:0}\n",
    "    \n",
    "    num_urls = 0\n",
    "    D= Downloader(delay=delay, user_agent=user_agent, num_retries=num_retries, cache=cache)#, timeout=timeout)\n",
    "    \n",
    "    def process_queue():\n",
    "        while True:\n",
    "            try:\n",
    "                url = crawl_queue.pop()\n",
    "            except IndexError:\n",
    "                # crawl queue is empty\n",
    "                break\n",
    "            else:\n",
    "                depth = seen[url]\n",
    "                html = D(url)\n",
    "                if depth != max_depth:\n",
    "                    for link in get_links(html):\n",
    "                        if re.match(link_regex, link):\n",
    "                            link = urllib.parse.urljoin(seed_url, link)\n",
    "                            if link not in seen:\n",
    "                                seen[link] = depth + 1\n",
    "                                crawl_queue.append(link)\n",
    "    threads = []\n",
    "    while threads or crawl_queue:\n",
    "        # the crawl is still active\n",
    "        for thread in threads:\n",
    "            if not thread.is_alive():\n",
    "                #remove the stopped threads\n",
    "                threads.remove(thread)\n",
    "        while len(threads) < max_threads and crawl_queue:\n",
    "            # can start some more threads\n",
    "            thread = threading.Thread(target=process_queue, daemon=True)\n",
    "            # set daemon so main thread can exit when receives ctrl-c\n",
    "            thread.start()\n",
    "            threads.append(thread)\n",
    "        # all threads have been processed\n",
    "        # sleep temporarily so CPU can focus execution elsewhere\n",
    "        time.sleep(SLEEP_TIME)\n",
    "    \n",
    "def get_links(html):\n",
    "    webpage_regex = re.compile('<a[^>]+href=[\"\\'](.*?)[\"\\']', re.IGNORECASE)\n",
    "    return webpage_regex.findall(html)\n",
    "client = MongoClient('localhost',27017)\n",
    "cache = MongoCache(client=client, expires=timedelta())\n",
    "threaded_crawler(seed_url='http://example.webscraping.com', link_regex='/(places/default/view|places/default/index)', \n",
    "              delay = 5, num_retries = 2,  max_depth = 3, cache=cache)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# 多进程爬虫\\n    # 由于爬虫队列保存在本地内存，其他进程无法处理这些爬虫\\n    # 因此将爬虫队列转移到MongoDB中，单独存储队列\\nfrom datetime import datetime, timedelta\\nfrom pymongo import MongoClient, errors\\n\\n# 添加了三种状态 （OUTSTANDING / PROCESSING / COMPLETE ）\\n    # OUTSTANDING：添加新URL\\n    # PROCESSING： 从队列取出URL准备下载\\n    # COMPLETE：   下载结束\\nclass MongoQueue:\\n    \\n    OUTSTANDING, PROCESSING, COMPLETE = range(3)\\n    \\n    def __init__(self, client=None, timeout=300):\\n        self.client = MongoClient()\\n        self.db = self.client.cache\\n        self.timeout = timeout\\n    \\n    def __nonzero__(self):\\n        # return true if there are more jobs to process\\n        record = self.db.crawl_queue.find_one( {'status': {'$ne': self.COMPLETE} } )\\n        return True if record else False\\n    \\n    def push(self, url):\\n        try:\\n            self.db.crawl_queue.insert( {'_id': url, 'status': self.OUTSTANDING} )\\n        except errors.DuplicateKeyError as e:\\n            # already in the queue\\n            pass\\n        \\n    def pop(self):\\n        record = self.db.crawl_queue.find_and_modify(\\n            query = { 'status': self.OUTSTANDING},\\n            update = {'$set': { 'status': self.PROCESSING, 'timestamp': datetime.now() } }\\n        )\\n        if record:\\n            return record['_id']\\n        else:\\n            self.repair()\\n            raise KeyError()\\n            \\n    def complete(self, url):\\n        self.db.crawl_queue.update( {'_id': url}, {'$set': {'status': self.COMPLETE} } )\\n    \\n    def repair(self):\\n        record = self.db.crawl_queue.find_and_modify(\\n            query = { \\n                'timestamp': { '$lt': datetime.now() - timedelta(seconds=self.timeout) },\\n                'status': { '$ne': self.COMPLETE}\\n            },\\n            update = {'$set': {'status': self.OUTSTANDING} }\\n        )\\n        if record:\\n            print('Released:', record['_id'])\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# 多进程爬虫\n",
    "    # 由于爬虫队列保存在本地内存，其他进程无法处理这些爬虫\n",
    "    # 因此将爬虫队列转移到MongoDB中，单独存储队列\n",
    "from datetime import datetime, timedelta\n",
    "from pymongo import MongoClient, errors\n",
    "\n",
    "# 添加了三种状态 （OUTSTANDING / PROCESSING / COMPLETE ）\n",
    "    # OUTSTANDING：添加新URL\n",
    "    # PROCESSING： 从队列取出URL准备下载\n",
    "    # COMPLETE：   下载结束\n",
    "class MongoQueue:\n",
    "    \n",
    "    OUTSTANDING, PROCESSING, COMPLETE = range(3)\n",
    "    \n",
    "    def __init__(self, client=None, timeout=300):\n",
    "        self.client = MongoClient()\n",
    "        self.db = self.client.cache\n",
    "        self.timeout = timeout\n",
    "    \n",
    "    def __nonzero__(self):\n",
    "        # return true if there are more jobs to process\n",
    "        record = self.db.crawl_queue.find_one( {'status': {'$ne': self.COMPLETE} } )\n",
    "        return True if record else False\n",
    "    \n",
    "    def push(self, url):\n",
    "        try:\n",
    "            self.db.crawl_queue.insert( {'_id': url, 'status': self.OUTSTANDING} )\n",
    "        except errors.DuplicateKeyError as e:\n",
    "            # already in the queue\n",
    "            pass\n",
    "        \n",
    "    def pop(self):\n",
    "        record = self.db.crawl_queue.find_and_modify(\n",
    "            query = { 'status': self.OUTSTANDING},\n",
    "            update = {'$set': { 'status': self.PROCESSING, 'timestamp': datetime.now() } }\n",
    "        )\n",
    "        if record:\n",
    "            return record['_id']\n",
    "        else:\n",
    "            self.repair()\n",
    "            raise KeyError()\n",
    "            \n",
    "    def complete(self, url):\n",
    "        self.db.crawl_queue.update( {'_id': url}, {'$set': {'status': self.COMPLETE} } )\n",
    "    \n",
    "    def repair(self):\n",
    "        record = self.db.crawl_queue.find_and_modify(\n",
    "            query = { \n",
    "                'timestamp': { '$lt': datetime.now() - timedelta(seconds=self.timeout) },\n",
    "                'status': { '$ne': self.COMPLETE}\n",
    "            },\n",
    "            update = {'$set': {'status': self.OUTSTANDING} }\n",
    "        )\n",
    "        if record:\n",
    "            print('Released:', record['_id'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 多进程爬虫\\n    # 修改爬虫队列，以存储在MongoDB中\\n        # 队列改为MongoQueue\\n        # seen标志取消，db已经实现id为url，因此不会重复下载\\n        \\n    \\n    # todo 虽然页面都下载完了，但是后来因为没有下载的了，就一直报KeyError，需要个终止条件\\n\\nimport time\\nimport threading\\nSLEEP_TIME = 1\\n\\ndef threaded_crawler2(seed_url, link_regex, num_retries=1, delay=5, cache=None , max_threads=10):\\n    \\n    crawl_queue = MongoQueue()\\n    crawl_queue.push(seed_url)\\n    \\n    \\n    num_urls = 0\\n    D= Downloader(delay=delay, user_agent=user_agent, num_retries=num_retries, cache=cache)#, timeout=timeout)\\n    \\n    \\n    def process_queue():\\n        while True:\\n            try:\\n                url = crawl_queue.pop()\\n            except IndexError:\\n                # crawl queue is empty\\n                break\\n            else:\\n                html = D(url)\\n                crawl_queue.complete(url)\\n                for link in get_links(html):\\n                    if re.match(link_regex, link):\\n                        link = urllib.parse.urljoin(seed_url, link)\\n                        crawl_queue.push(link)\\n    threads = []\\n    while threads or crawl_queue:\\n        # the crawl is still active\\n        for thread in threads:\\n            if not thread.is_alive():\\n                #remove the stopped threads\\n                threads.remove(thread)\\n        while len(threads) < max_threads and crawl_queue:\\n            # can start some more threads\\n            thread = threading.Thread(target=process_queue, daemon=True)\\n            # set daemon so main thread can exit when receives ctrl-c\\n            thread.start()\\n            threads.append(thread)\\n        # all threads have been processed\\n        # sleep temporarily so CPU can focus execution elsewhere\\n        time.sleep(SLEEP_TIME)\\n    \\ndef get_links(html):\\n    webpage_regex = re.compile(\\'<a[^>]+href=[\"\\'](.*?)[\"\\']\\', re.IGNORECASE)\\n    return webpage_regex.findall(html)\\n\\nclient = MongoClient(\\'localhost\\',27017)\\ncache = MongoCache(client=client, expires=timedelta())\\nthreaded_crawler2(seed_url=\\'http://example.webscraping.com\\', link_regex=\\'/(places/default/view|places/default/index)\\', \\n              delay = 5, num_retries = 2, cache=cache)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# 多进程爬虫\n",
    "    # 修改爬虫队列，以存储在MongoDB中\n",
    "        # 队列改为MongoQueue\n",
    "        # seen标志取消，db已经实现id为url，因此不会重复下载\n",
    "        \n",
    "    \n",
    "    # todo 虽然页面都下载完了，但是后来因为没有下载的了，就一直报KeyError，需要个终止条件\n",
    "\n",
    "import time\n",
    "import threading\n",
    "SLEEP_TIME = 1\n",
    "\n",
    "def threaded_crawler2(seed_url, link_regex, num_retries=1, delay=5, cache=None , max_threads=10):\n",
    "    \n",
    "    crawl_queue = MongoQueue()\n",
    "    crawl_queue.push(seed_url)\n",
    "    \n",
    "    \n",
    "    num_urls = 0\n",
    "    D= Downloader(delay=delay, user_agent=user_agent, num_retries=num_retries, cache=cache)#, timeout=timeout)\n",
    "    \n",
    "    \n",
    "    def process_queue():\n",
    "        while True:\n",
    "            try:\n",
    "                url = crawl_queue.pop()\n",
    "            except IndexError:\n",
    "                # crawl queue is empty\n",
    "                break\n",
    "            else:\n",
    "                html = D(url)\n",
    "                crawl_queue.complete(url)\n",
    "                for link in get_links(html):\n",
    "                    if re.match(link_regex, link):\n",
    "                        link = urllib.parse.urljoin(seed_url, link)\n",
    "                        crawl_queue.push(link)\n",
    "    threads = []\n",
    "    while threads or crawl_queue:\n",
    "        # the crawl is still active\n",
    "        for thread in threads:\n",
    "            if not thread.is_alive():\n",
    "                #remove the stopped threads\n",
    "                threads.remove(thread)\n",
    "        while len(threads) < max_threads and crawl_queue:\n",
    "            # can start some more threads\n",
    "            thread = threading.Thread(target=process_queue, daemon=True)\n",
    "            # set daemon so main thread can exit when receives ctrl-c\n",
    "            thread.start()\n",
    "            threads.append(thread)\n",
    "        # all threads have been processed\n",
    "        # sleep temporarily so CPU can focus execution elsewhere\n",
    "        time.sleep(SLEEP_TIME)\n",
    "    \n",
    "def get_links(html):\n",
    "    webpage_regex = re.compile('<a[^>]+href=[\"\\'](.*?)[\"\\']', re.IGNORECASE)\n",
    "    return webpage_regex.findall(html)\n",
    "\n",
    "client = MongoClient('localhost',27017)\n",
    "cache = MongoCache(client=client, expires=timedelta())\n",
    "threaded_crawler2(seed_url='http://example.webscraping.com', link_regex='/(places/default/view|places/default/index)', \n",
    "              delay = 5, num_retries = 2, cache=cache)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# 启动多个进程\\nimport multiprocessing\\n\\ndef process_link_crawler(args, **kwargs):\\n    \\n    num_cpus = multiprocessing.cpu_count()\\n    print('Starting {} processes'.format(num_cpus))\\n    processes = []\\n    \\n    for i in range(num_cpus):\\n        p = multiprocessing.Process(target=threaded_crawler, args=[args], kwargs=kwargs)\\n        p.start()\\n        processes.append(p)\\n    # wait for processes to complete\\n    for p in processes:\\n        p.join()\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# 启动多个进程\n",
    "import multiprocessing\n",
    "\n",
    "def process_link_crawler(args, **kwargs):\n",
    "    \n",
    "    num_cpus = multiprocessing.cpu_count()\n",
    "    print('Starting {} processes'.format(num_cpus))\n",
    "    processes = []\n",
    "    \n",
    "    for i in range(num_cpus):\n",
    "        p = multiprocessing.Process(target=threaded_crawler, args=[args], kwargs=kwargs)\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "    # wait for processes to complete\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "'''    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
