{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloader类\n",
    "    \n",
    "import urllib\n",
    "import time\n",
    "class Downloader:\n",
    "    def __init__(self, delay=5, user_agent='yuhao', num_retries=1, cache=None):\n",
    "        self.throttle = Throttle(delay)\n",
    "        self.user_agent = user_agent\n",
    "        self.num_retries = num_retries\n",
    "        self.cache = cache\n",
    "    def __call__(self, url):\n",
    "        result = None\n",
    "        if self.cache:\n",
    "            try:\n",
    "                result = self.cache[url]\n",
    "            except KeyError:\n",
    "                #url is not available in cache\n",
    "                pass\n",
    "            else:\n",
    "                if result['code'] == None:\n",
    "                     return result['html']\n",
    "                if self.num_retries > 0 and 500 <= result['code'] <600:\n",
    "                    result = None\n",
    "        if result is None:\n",
    "            self.throttle.wait(url)\n",
    "            headers = {'User-agent':self.user_agent}\n",
    "            result = self.download(url, headers, self.num_retries)\n",
    "            if self.cache:\n",
    "                self.cache[url] = result\n",
    "            return result['html']\n",
    "    def download(self, url, headers, num_retries, data=None):\n",
    "        print ('Downloading:',url)\n",
    "        # 创建请求而不是url， url读取会被禁止, 设置代理(默认代理会被拒)\n",
    "        req = urllib.request.Request(url, headers={'User-agent':self.user_agent})\n",
    "        code = None\n",
    "        try:\n",
    "            # 读取后编码成字符串，在后面re.findall中才能匹配\n",
    "            html = urllib.request.urlopen(req).read().decode('utf-8')\n",
    "\n",
    "        except urllib.error.HTTPError as e:\n",
    "            print ('Download error:',e.reason)\n",
    "            html = None\n",
    "            code = e.code\n",
    "            if(num_retries > 0):\n",
    "                # 处理500-600的服务器错误\n",
    "                if hasattr(e, 'code') and 500 <= e.code <600:\n",
    "                    return download(url, num_retries - 1)\n",
    "        \n",
    "        return {'html':html,'code':code}\n",
    "        \n",
    "\n",
    "\n",
    "import re\n",
    "from datetime import datetime\n",
    "#import csv\n",
    "#import urllib.robotparser as r_p\n",
    "#import time\n",
    "#import lxml.html\n",
    "\n",
    "# Throttle类记录每个域名最近访问时间\n",
    "class Throttle:\n",
    "    def __init__(self, delay):\n",
    "        self.delay = delay\n",
    "        self.domains = {}\n",
    "    def wait(self, url):\n",
    "        domain = urllib.parse.urlparse(url).netloc\n",
    "        last_accessed = self.domains.get(domain)\n",
    "        if self.delay > 0 and last_accessed is not None:\n",
    "            sleep_secs = self.delay - (datetime.now() - last_accessed).seconds\n",
    "            if sleep_secs > 0:\n",
    "                time.sleep(sleep_secs)\n",
    "        self.domains[domain] = datetime.now()    \n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: http://example.webscraping.com/places/default/search\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 动态网页，直接读取的结果为空\n",
    "import lxml.html\n",
    "D = Downloader()\n",
    "html = D('http://example.webscraping.com/places/default/search')\n",
    "tree = lxml.html.fromstring(html)\n",
    "tree.cssselect('div#results a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: http://example.webscraping.com/places/ajax/search.json?page=0&page_size=10&search_term=a\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'records': [{'pretty_link': '<div><a href=\"/places/default/view/Afghanistan-1\"><img src=\"/places/static/images/flags/af.png\" /> Afghanistan</a></div>',\n",
       "   'country': 'Afghanistan',\n",
       "   'id': 7049953},\n",
       "  {'pretty_link': '<div><a href=\"/places/default/view/Aland-Islands-2\"><img src=\"/places/static/images/flags/ax.png\" /> Aland Islands</a></div>',\n",
       "   'country': 'Aland Islands',\n",
       "   'id': 7049954},\n",
       "  {'pretty_link': '<div><a href=\"/places/default/view/Albania-3\"><img src=\"/places/static/images/flags/al.png\" /> Albania</a></div>',\n",
       "   'country': 'Albania',\n",
       "   'id': 7049955},\n",
       "  {'pretty_link': '<div><a href=\"/places/default/view/Algeria-4\"><img src=\"/places/static/images/flags/dz.png\" /> Algeria</a></div>',\n",
       "   'country': 'Algeria',\n",
       "   'id': 7049956},\n",
       "  {'pretty_link': '<div><a href=\"/places/default/view/American-Samoa-5\"><img src=\"/places/static/images/flags/as.png\" /> American Samoa</a></div>',\n",
       "   'country': 'American Samoa',\n",
       "   'id': 7049957},\n",
       "  {'pretty_link': '<div><a href=\"/places/default/view/Andorra-6\"><img src=\"/places/static/images/flags/ad.png\" /> Andorra</a></div>',\n",
       "   'country': 'Andorra',\n",
       "   'id': 7049958},\n",
       "  {'pretty_link': '<div><a href=\"/places/default/view/Angola-7\"><img src=\"/places/static/images/flags/ao.png\" /> Angola</a></div>',\n",
       "   'country': 'Angola',\n",
       "   'id': 7049959},\n",
       "  {'pretty_link': '<div><a href=\"/places/default/view/Anguilla-8\"><img src=\"/places/static/images/flags/ai.png\" /> Anguilla</a></div>',\n",
       "   'country': 'Anguilla',\n",
       "   'id': 7049960},\n",
       "  {'pretty_link': '<div><a href=\"/places/default/view/Antarctica-9\"><img src=\"/places/static/images/flags/aq.png\" /> Antarctica</a></div>',\n",
       "   'country': 'Antarctica',\n",
       "   'id': 7049961},\n",
       "  {'pretty_link': '<div><a href=\"/places/default/view/Antigua-and-Barbuda-10\"><img src=\"/places/static/images/flags/ag.png\" /> Antigua and Barbuda</a></div>',\n",
       "   'country': 'Antigua and Barbuda',\n",
       "   'id': 7049962}],\n",
       " 'num_pages': 22,\n",
       " 'error': ''}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看AJAX请求结果（json格式）\n",
    "html = D('http://example.webscraping.com/places/ajax/search.json?page=0&page_size=10&search_term=a')\n",
    "#print(html)\n",
    "import json\n",
    "json.loads(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'string' has no attribute 'lowercase'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-4d53e7d1616e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcountries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mletter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mpage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'string' has no attribute 'lowercase'"
     ]
    }
   ],
   "source": [
    "# 剥离国家信息，通过26次ajax请求下载页面\n",
    "import json\n",
    "import string\n",
    "template_url = 'http://example.webscraping.com/places/ajax/search.json?page={}&page_size=10&search_term={}'\n",
    "countries = set()\n",
    "\n",
    "for letter in string.ascii_lowercase:\n",
    "    page = 0\n",
    "    while True:\n",
    "        html = D(template_url.format(page, letter))\n",
    "        try:\n",
    "            ajax = json.loads(html)\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            ajax = None\n",
    "        else:\n",
    "            for reord in ajax['records']:\n",
    "                countries.add(record['country'])\n",
    "        page += 1\n",
    "        if ajax is None or page >= ajax['num_pages']:\n",
    "            break\n",
    "    \n",
    "open('countries.txt','w').write('\\n'.join(sorted(countries)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 观察发现search页面是通过正则匹配字符\n",
    "json.loads(D(url + '.'))['num_pages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
