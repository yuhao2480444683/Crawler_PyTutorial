{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloader类\n",
    "    \n",
    "import urllib\n",
    "import time\n",
    "class Downloader:\n",
    "    def __init__(self, delay=5, user_agent='yuhao', num_retries=1, cache=None):\n",
    "        self.throttle = Throttle(delay)\n",
    "        self.user_agent = user_agent\n",
    "        self.num_retries = num_retries\n",
    "        self.cache = cache\n",
    "    def __call__(self, url):\n",
    "        result = None\n",
    "        if self.cache:\n",
    "            try:\n",
    "                result = self.cache[url]\n",
    "            except KeyError:\n",
    "                #url is not available in cache\n",
    "                pass\n",
    "            else:\n",
    "                if result['code'] == None:\n",
    "                     return result['html']\n",
    "                if self.num_retries > 0 and 500 <= result['code'] <600:\n",
    "                    result = None\n",
    "        if result is None:\n",
    "            self.throttle.wait(url)\n",
    "            headers = {'User-agent':self.user_agent}\n",
    "            result = self.download(url, headers, self.num_retries)\n",
    "            if self.cache:\n",
    "                self.cache[url] = result\n",
    "            return result['html']\n",
    "    def download(self, url, headers, num_retries, data=None):\n",
    "        print ('Downloading:',url)\n",
    "        # 创建请求而不是url， url读取会被禁止, 设置代理(默认代理会被拒)\n",
    "        req = urllib.request.Request(url, headers={'User-agent':self.user_agent})\n",
    "        code = None\n",
    "        try:\n",
    "            # 读取后编码成字符串，在后面re.findall中才能匹配\n",
    "            html = urllib.request.urlopen(req).read().decode('utf-8')\n",
    "\n",
    "        except urllib.error.HTTPError as e:\n",
    "            print ('Download error:',e.reason)\n",
    "            html = None\n",
    "            code = e.code\n",
    "            if(num_retries > 0):\n",
    "                # 处理500-600的服务器错误\n",
    "                if hasattr(e, 'code') and 500 <= e.code <600:\n",
    "                    return download(url, num_retries - 1)\n",
    "        \n",
    "        return {'html':html,'code':code}\n",
    "        \n",
    "\n",
    "\n",
    "import re\n",
    "from datetime import datetime\n",
    "#import csv\n",
    "#import urllib.robotparser as r_p\n",
    "#import time\n",
    "#import lxml.html\n",
    "\n",
    "# Throttle类记录每个域名最近访问时间\n",
    "class Throttle:\n",
    "    def __init__(self, delay):\n",
    "        self.delay = delay\n",
    "        self.domains = {}\n",
    "    def wait(self, url):\n",
    "        domain = urllib.parse.urlparse(url).netloc\n",
    "        last_accessed = self.domains.get(domain)\n",
    "        if self.delay > 0 and last_accessed is not None:\n",
    "            sleep_secs = self.delay - (datetime.now() - last_accessed).seconds\n",
    "            if sleep_secs > 0:\n",
    "                time.sleep(sleep_secs)\n",
    "        self.domains[domain] = datetime.now()    \n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: http://example.webscraping.com/places/default/search\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 动态网页，直接读取的结果为空\n",
    "import lxml.html\n",
    "D = Downloader()\n",
    "html = D('http://example.webscraping.com/places/default/search')\n",
    "tree = lxml.html.fromstring(html)\n",
    "tree.cssselect('div#results a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: http://example.webscraping.com/places/ajax/search.json?page=0&page_size=10&search_term=a\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'records': [{'pretty_link': '<div><a href=\"/places/default/view/Afghanistan-1\"><img src=\"/places/static/images/flags/af.png\" /> Afghanistan</a></div>',\n",
       "   'country': 'Afghanistan',\n",
       "   'id': 7050709},\n",
       "  {'pretty_link': '<div><a href=\"/places/default/view/Aland-Islands-2\"><img src=\"/places/static/images/flags/ax.png\" /> Aland Islands</a></div>',\n",
       "   'country': 'Aland Islands',\n",
       "   'id': 7050710},\n",
       "  {'pretty_link': '<div><a href=\"/places/default/view/Albania-3\"><img src=\"/places/static/images/flags/al.png\" /> Albania</a></div>',\n",
       "   'country': 'Albania',\n",
       "   'id': 7050711},\n",
       "  {'pretty_link': '<div><a href=\"/places/default/view/Algeria-4\"><img src=\"/places/static/images/flags/dz.png\" /> Algeria</a></div>',\n",
       "   'country': 'Algeria',\n",
       "   'id': 7050712},\n",
       "  {'pretty_link': '<div><a href=\"/places/default/view/American-Samoa-5\"><img src=\"/places/static/images/flags/as.png\" /> American Samoa</a></div>',\n",
       "   'country': 'American Samoa',\n",
       "   'id': 7050713},\n",
       "  {'pretty_link': '<div><a href=\"/places/default/view/Andorra-6\"><img src=\"/places/static/images/flags/ad.png\" /> Andorra</a></div>',\n",
       "   'country': 'Andorra',\n",
       "   'id': 7050714},\n",
       "  {'pretty_link': '<div><a href=\"/places/default/view/Angola-7\"><img src=\"/places/static/images/flags/ao.png\" /> Angola</a></div>',\n",
       "   'country': 'Angola',\n",
       "   'id': 7050715},\n",
       "  {'pretty_link': '<div><a href=\"/places/default/view/Anguilla-8\"><img src=\"/places/static/images/flags/ai.png\" /> Anguilla</a></div>',\n",
       "   'country': 'Anguilla',\n",
       "   'id': 7050716},\n",
       "  {'pretty_link': '<div><a href=\"/places/default/view/Antarctica-9\"><img src=\"/places/static/images/flags/aq.png\" /> Antarctica</a></div>',\n",
       "   'country': 'Antarctica',\n",
       "   'id': 7050717},\n",
       "  {'pretty_link': '<div><a href=\"/places/default/view/Antigua-and-Barbuda-10\"><img src=\"/places/static/images/flags/ag.png\" /> Antigua and Barbuda</a></div>',\n",
       "   'country': 'Antigua and Barbuda',\n",
       "   'id': 7050718}],\n",
       " 'num_pages': 22,\n",
       " 'error': ''}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看AJAX请求结果（json格式）\n",
    "html = D('http://example.webscraping.com/places/ajax/search.json?page=0&page_size=10&search_term=a')\n",
    "#print(html)\n",
    "import json\n",
    "json.loads(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# 剥离国家信息，通过26次ajax请求下载页面\\nimport json\\nimport string\\ntemplate_url = 'http://example.webscraping.com/places/ajax/search.json?page={}&page_size=10&search_term={}'\\ncountries = set()\\n\\nfor letter in string.ascii_lowercase:\\n    page = 0\\n    while True:\\n        html = D(template_url.format(page, letter))\\n        try:\\n            ajax = json.loads(html)\\n        except ValueError as e:\\n            print(e)\\n            ajax = None\\n        else:\\n            for record in ajax['records']:\\n                countries.add(record['country'])\\n        page += 1\\n        if ajax is None or page >= ajax['num_pages']:\\n            break\\n    \\nopen('countries.txt','w').write('\\n'.join(sorted(countries)))\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# 剥离国家信息，通过26次ajax请求下载页面\n",
    "import json\n",
    "import string\n",
    "template_url = 'http://example.webscraping.com/places/ajax/search.json?page={}&page_size=10&search_term={}'\n",
    "countries = set()\n",
    "\n",
    "for letter in string.ascii_lowercase:\n",
    "    page = 0\n",
    "    while True:\n",
    "        html = D(template_url.format(page, letter))\n",
    "        try:\n",
    "            ajax = json.loads(html)\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            ajax = None\n",
    "        else:\n",
    "            for record in ajax['records']:\n",
    "                countries.add(record['country'])\n",
    "        page += 1\n",
    "        if ajax is None or page >= ajax['num_pages']:\n",
    "            break\n",
    "    \n",
    "open('countries.txt','w').write('\\n'.join(sorted(countries)))\n",
    "'''      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: http://example.webscraping.com/places/ajax/search.json?page=0&page_size=10&search_term=.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 观察发现search页面是通过正则匹配字符\n",
    "url = 'http://example.webscraping.com/places/ajax/search.json?page=0&page_size=10&search_term='\n",
    "json.loads(D(url + '.'))['num_pages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'drive' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-1bbb274c2b06>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# 关闭浏览器\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mdrive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'drive' is not defined"
     ]
    }
   ],
   "source": [
    "# 使用Selenium渲染网页，更加简便简单\n",
    "    # 需要先下载Chrome webdriver\n",
    "        # link： http://chromedriver.storage.googleapis.com/index.html\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "# 在driver中显示页面\n",
    "driver.get('http://example.webscraping.com/places/default/search')\n",
    "# 模拟键盘输入\n",
    "driver.find_element_by_id('search_term').send_keys('.')\n",
    "\n",
    "# 使用javascript语句直接设置选项框内容\n",
    "js = \"document.getElementById('page_size').options[1].text='1000'\"\n",
    "driver.execute_script(js)\n",
    "\n",
    "driver.find_element_by_id('search').click()\n",
    "\n",
    "# 等待AJAX请求完成，至多等待30秒\n",
    "driver.implicitly_wait(30)\n",
    "\n",
    "# 关闭浏览器\n",
    "driver.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
