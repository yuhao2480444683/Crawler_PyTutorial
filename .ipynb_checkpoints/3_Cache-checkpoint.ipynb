{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download函数\n",
    "    # 添加缓存机制\n",
    "    # 限速添加至下载类中，读取缓存则不用限速\n",
    "    \n",
    "import urllib\n",
    "class Downloader:\n",
    "    def __init__(self, delay=5, user_agent='yuhao', num_retries=1, cache=None):\n",
    "        self.throttle = Throttle(delay)\n",
    "        self.user_agent = user_agent\n",
    "        self.num_retries = num_retries\n",
    "        self.cache = cache\n",
    "    def __call__(self, url):\n",
    "        result = None\n",
    "        if self.cache:\n",
    "            try:\n",
    "                result = self.cache[url]\n",
    "            except KeyError:\n",
    "                #url is not available in cache\n",
    "                pass\n",
    "            else:\n",
    "                if result['code'] == None:\n",
    "                     return result['html']\n",
    "                if self.num_retries > 0 and 500 <= result['code'] <600:\n",
    "                    result = None\n",
    "        if result is None:\n",
    "            self.throttle.wait(url)\n",
    "            headers = {'User-agent':self.user_agent}\n",
    "            result = self.download(url, headers, self.num_retries)\n",
    "            if self.cache:\n",
    "                self.cache[url] = result\n",
    "            return result['html']\n",
    "    def download(self, url, headers, num_retries, data=None):\n",
    "        print ('Downloading:',url)\n",
    "        # 创建请求而不是url， url读取会被禁止, 设置代理(默认代理会被拒)\n",
    "        req = urllib.request.Request(url, headers={'User-agent':user_agent})\n",
    "        code = None\n",
    "        try:\n",
    "            # 读取后编码成字符串，在后面re.findall中才能匹配\n",
    "            html = urllib.request.urlopen(req).read().decode('utf-8')\n",
    "\n",
    "        except urllib.error.HTTPError as e:\n",
    "            print ('Download error:',e.reason)\n",
    "            html = None\n",
    "            code = e.code\n",
    "            if(num_retries > 0):\n",
    "                # 处理500-600的服务器错误\n",
    "                if hasattr(e, 'code') and 500 <= e.code <600:\n",
    "                    return download(url, num_retries - 1)\n",
    "        \n",
    "        return {'html':html,'code':code}\n",
    "        \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\nlink_crawler4('http://example.webscraping.com', '/(places/default/view|places/default/index)', \\n              delay = 3, num_retries = 2,  max_depth = -1)\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 链接爬虫4.0\n",
    "   # 加入缓存\n",
    "    \n",
    "    \n",
    "# 爬虫限速\n",
    "# Throttle类记录每个域名最近访问时间\n",
    "class Throttle:\n",
    "    def __init__(self, delay):\n",
    "        self.delay = delay\n",
    "        self.domains = {}\n",
    "    def wait(self, url):\n",
    "        domain = urllib.parse.urlparse(url).netloc\n",
    "        last_accessed = self.domains.get(domain)\n",
    "        if self.delay > 0 and last_accessed is not None:\n",
    "            sleep_secs = self.delay - (datetime.now() - last_accessed).seconds\n",
    "            if sleep_secs > 0:\n",
    "                time.sleep(sleep_secs)\n",
    "        self.domains[domain] = datetime.now()    \n",
    "\n",
    "        \n",
    "import re\n",
    "import datetime\n",
    "import csv\n",
    "import urllib.robotparser as r_p\n",
    "import time\n",
    "import lxml.html\n",
    "\n",
    "rp = r_p.RobotFileParser()\n",
    "rp.set_url('http://example.webscraping.com/robots.txt')\n",
    "rp.read()\n",
    "\n",
    "user_agent = 'GoodCrawler'\n",
    "\n",
    "\n",
    "\n",
    "# 链接爬取\n",
    "def link_crawler4(seed_url, link_regex, num_retries=1, delay=5, max_depth=2, cache=None):\n",
    "    max_depth = 2\n",
    "    crawl_queue = [seed_url]\n",
    "    # 避免重复url\n",
    "    seen = {seed_url:0}\n",
    "    \n",
    "    num_urls = 0\n",
    "    D= Downloader(delay=delay, user_agent=user_agent, num_retries=num_retries, cache=cache)\n",
    "    \n",
    "    while crawl_queue:\n",
    "        url = crawl_queue.pop()\n",
    "        depth = seen[url]\n",
    "        print( rp.can_fetch(user_agent, url))\n",
    "        # 检查robots.txt是否当前代理可以爬取\n",
    "        if not rp.can_fetch(user_agent, url):\n",
    "            print(\"Blocked by robots.txt\", user_agent, url)\n",
    "            return\n",
    "        html = D(url)\n",
    "        \n",
    "        if depth != max_depth:\n",
    "            for link in get_links(html):\n",
    "                if re.match(link_regex, link):\n",
    "                    link = urllib.parse.urljoin(seed_url, link)\n",
    "                    if link not in seen:\n",
    "                        seen[link] = depth + 1\n",
    "                        crawl_queue.append(link)\n",
    "\n",
    "def get_links(html):\n",
    "    webpage_regex = re.compile('<a[^>]+href=[\"\\'](.*?)[\"\\']', re.IGNORECASE)\n",
    "    return webpage_regex.findall(html)\n",
    "\n",
    "\n",
    "''' \n",
    "link_crawler4('http://example.webscraping.com', '/(places/default/view|places/default/index)', \n",
    "              delay = 3, num_retries = 2,  max_depth = -1)\n",
    "'''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n# pymongo test\\nfrom pymongo import MongoClient\\nclient = MongoClient(\\'localhost\\',27017)\\n\\nurl = \\'http://example.webscraping.com/view/Antigua-and-Barbuda-10\\'\\nhtml = \\'...\\'\\ndb = client.cache\\n#db.webpage.insert_one({\"url\":url, \\'html\\':html})\\n#db.webpage.insert_one({\"url\":url + \\'???\\',\\'html\\':\"???\"})\\ndb.webpage.find({\"url\":url}).count()\\n\\n#db.webpage.delete_many({})\\n#db.webpage.update_one({\\'_id\\': url}, { \"$set\": { \"html\": \"3\"} }, upsert=True)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "\n",
    "# pymongo test\n",
    "from pymongo import MongoClient\n",
    "client = MongoClient('localhost',27017)\n",
    "\n",
    "url = 'http://example.webscraping.com/view/Antigua-and-Barbuda-10'\n",
    "html = '...'\n",
    "db = client.cache\n",
    "#db.webpage.insert_one({\"url\":url, 'html':html})\n",
    "#db.webpage.insert_one({\"url\":url + '???','html':\"???\"})\n",
    "db.webpage.find({\"url\":url}).count()\n",
    "\n",
    "#db.webpage.delete_many({})\n",
    "#db.webpage.update_one({'_id': url}, { \"$set\": { \"html\": \"3\"} }, upsert=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# 使用MongoDB实现cache类\n",
    "    # 创建index时，expireAfterSeconds属性如果为utc时间，则会被数据库自动删除，否则不会\n",
    "from datetime import datetime, timedelta\n",
    "from pymongo import MongoClient\n",
    "\n",
    "class MongoCache:\n",
    "    def __init__(self, client=None, expires=timedelta(days=30)):\n",
    "        self.client = MongoClient('localhost',27017)\n",
    "        self.db = client.cache\n",
    "        self.db.webpage.create_index('timestamp', expireAfterSeconds=expires.total_seconds())\n",
    "    \n",
    "    def __getitem__(self, url):\n",
    "        record = self.db.webpage.find_one({'_id':url})\n",
    "        if record:\n",
    "            return record['result']\n",
    "        else:\n",
    "            raise KeyError(url + 'does not exist')\n",
    "            \n",
    "    def __setitem__(self, url, result):\n",
    "        record = {'result': result, 'timestamp':datetime.now()}\n",
    "        self.db.webpage.update( {'_id': url}, {'$set': record},  upsert=True)\n",
    "\n",
    "client = MongoClient('localhost',27017)\n",
    "cache = MongoCache(client=client, expires=timedelta())\n",
    "\n",
    "\n",
    "\n",
    "link_crawler4('http://example.webscraping.com', '/(places/default/view|places/default/index)', \n",
    "              delay = 3, cache = cache, num_retries = 2,  max_depth = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# 压缩数据库\\nimport pickle\\nimport zlib\\nfrom bson.binary import Binary\\n\\nclass MongoCache:\\n    def __init__(self, client=None, expires=timedelta(days=30)):\\n        self.client = MongoClient('localhost',27017)\\n        self.db = client.cache\\n        self.db.webpage.create_index('timestamp', expireAfterSeconds=expires.total_seconds())\\n    \\n    def __getitem__(self, url):\\n        record = self.db.webpage.find_one({'_id':url})\\n        if record:\\n            return pickle.loads(zlib.decompress(record['result']))\\n        else:\\n            raise KeyError(url + 'does not exist')\\n            \\n    def __setitem__(self, url, result):\\n        record = {'result': Binary(zlib.compress(pickle.dumps(result))),\\n                  'timestamp':datetime.utcnow()}\\n        self.db.webpage.update( {'_id': url}, {'$set': record},  upsert=True)\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# 压缩数据库\n",
    "import pickle\n",
    "import zlib\n",
    "from bson.binary import Binary\n",
    "\n",
    "class MongoCache:\n",
    "    def __init__(self, client=None, expires=timedelta(days=30)):\n",
    "        self.client = MongoClient('localhost',27017)\n",
    "        self.db = client.cache\n",
    "        self.db.webpage.create_index('timestamp', expireAfterSeconds=expires.total_seconds())\n",
    "    \n",
    "    def __getitem__(self, url):\n",
    "        record = self.db.webpage.find_one({'_id':url})\n",
    "        if record:\n",
    "            return pickle.loads(zlib.decompress(record['result']))\n",
    "        else:\n",
    "            raise KeyError(url + 'does not exist')\n",
    "            \n",
    "    def __setitem__(self, url, result):\n",
    "        record = {'result': Binary(zlib.compress(pickle.dumps(result))),\n",
    "                  'timestamp':datetime.utcnow()}\n",
    "        self.db.webpage.update( {'_id': url}, {'$set': record},  upsert=True)\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
